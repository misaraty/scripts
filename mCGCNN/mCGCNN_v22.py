import os
import time
import warnings
from dataclasses import dataclass, asdict
from typing import List, Optional
import csv, functools, json, random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import r2_score
from torch.optim.lr_scheduler import MultiStepLR, StepLR
from pymatgen.core.structure import Structure
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.dataloader import default_collate
from torch.utils.data.sampler import SubsetRandomSampler
from sklearn.model_selection import train_test_split, KFold
import optuna

warnings.filterwarnings("ignore")
SCRIPT_DIR = os.path.split(os.path.realpath(__file__))[0]
os.chdir(SCRIPT_DIR)
# ----------------------------- Parameters -----------------------------
CIFDATA_VARIANT = "edge"  # 'origin', 'atom', 'edge'
DELTA_EN_FEAT_MODE = "rbf"  # 'raw' | 'poly' | 'rbf' | 'fourier' | 'all'
DELTA_EN_FOURIER_K = 3  # Number of sin/cos frequency components K in Fourier mode
DELTA_EN_RBF_GAMMA = 2.0  # Gamma parameter in RBF mode (coefficient for Δχ^2)

n_folds = 5  # 'none' | int
train_ratio = 0.7
val_ratio = 0.0
test_ratio = 0.3
batch_size = 256
lr = 1e-3
patience = 200
epochs = 200
print_freq = 10
USE_OPTUNA = True
OPTUNA_TRIALS = 20
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
STRUCT_DIR = os.path.join(SCRIPT_DIR, "cif")
META_FILE = os.path.join(SCRIPT_DIR, "data.xlsx")
ID_COL = "cif"
TARGET_COL = "tc"
# ----------------------------- Data utils -----------------------------
ATOM_INIT_JSON_STR = r"""{"1": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "2": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "3": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "4": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "5": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "6": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], "7": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "8": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "9": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "10": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "11": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "12": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "13": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "14": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "15": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "16": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "17": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "18": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "19": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "20": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "21": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "22": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "23": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "24": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "25": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "26": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "27": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "28": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "29": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], "30": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "31": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "32": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "33": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "34": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "35": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "36": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "37": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "38": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "39": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "40": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "41": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "42": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "43": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "44": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "45": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "46": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "47": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "48": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "49": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "50": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "51": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "52": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "53": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "54": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "55": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "56": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], "57": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "58": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "59": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "60": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "61": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "62": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "63": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "64": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "65": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "66": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "67": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "68": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "69": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "70": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], "71": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "72": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "73": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "74": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "75": [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "76": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "77": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "78": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "79": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], "80": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "81": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "82": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "83": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "84": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "85": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "86": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "87": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "88": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], "89": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "90": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "91": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], "92": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], "93": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "94": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "95": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "96": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], "97": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "98": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "99": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "100": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"""
with open("atom_init.json", "w", encoding="utf-8") as f:
    f.write(ATOM_INIT_JSON_STR)
ATOM_INIT_PATH = os.path.join(SCRIPT_DIR, "atom_init.json")


def get_train_val_test_loader(
    dataset,
    collate_fn=default_collate,
    batch_size=batch_size,
    train_ratio=train_ratio,
    val_ratio=val_ratio,
    test_ratio=test_ratio,
    return_test=False,
    num_workers=0,
    pin_memory=False,
    **kwargs,
):
    total_size = len(dataset)
    if kwargs.get("train_size") is None:
        if train_ratio is None:
            assert val_ratio + test_ratio < 1
            train_ratio = 1 - val_ratio - test_ratio
            print(
                f"[Warning] train_ratio is None, using 1 - val_ratio - test_ratio = {train_ratio} as training data."
            )
        else:
            assert train_ratio + val_ratio + test_ratio <= 1
    train_size = (
        kwargs.get("train_size")
        if kwargs.get("train_size") is not None
        else int(round(train_ratio * total_size))
    )
    test_size = (
        kwargs.get("test_size")
        if kwargs.get("test_size") is not None
        else int(round(test_ratio * total_size))
    )
    valid_size = (
        kwargs.get("val_size")
        if kwargs.get("val_size") is not None
        else int(round(val_ratio * total_size))
    )
    assert (
        train_size + valid_size + test_size <= total_size
    ), "Split sizes exceed dataset length."
    idx = np.arange(total_size)
    train_idx, temp_idx = train_test_split(
        idx, test_size=total_size - train_size, shuffle=True, random_state=seed
    )
    if n_folds == "none":
        if (valid_size + test_size) > 0:
            val_prop = valid_size / float(valid_size + test_size)
            val_idx, test_idx = train_test_split(
                temp_idx, test_size=(1.0 - val_prop), shuffle=True, random_state=seed
            )
        else:
            val_idx, test_idx = np.array([], dtype=int), np.array([], dtype=int)
    else:
        if (valid_size + test_size) > 0:
            if valid_size == 0:
                val_idx = np.array([], dtype=int)
                test_idx = temp_idx
            elif test_size == 0:
                val_idx = temp_idx
                test_idx = np.array([], dtype=int)
            else:
                val_prop = valid_size / float(valid_size + test_size)
                test_prop = 1.0 - val_prop
                val_idx, test_idx = train_test_split(
                    temp_idx, test_size=test_prop, shuffle=True, random_state=seed
                )
        else:
            val_idx, test_idx = np.array([], dtype=int), np.array([], dtype=int)
    train_sampler = SubsetRandomSampler(train_idx.tolist())
    val_sampler = SubsetRandomSampler(val_idx.tolist())
    test_sampler = SubsetRandomSampler(test_idx.tolist()) if return_test else None
    train_loader = DataLoader(
        dataset,
        batch_size=min(batch_size, max(1, len(train_idx))),
        sampler=train_sampler,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=pin_memory,
    )
    val_loader = DataLoader(
        dataset,
        batch_size=min(batch_size, max(1, len(val_idx))),
        sampler=val_sampler,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=pin_memory,
    )
    if return_test:
        test_loader = DataLoader(
            dataset,
            batch_size=min(batch_size, max(1, len(test_idx))),
            sampler=test_sampler,
            num_workers=num_workers,
            collate_fn=collate_fn,
            pin_memory=pin_memory,
        )
        return train_loader, val_loader, test_loader
    return train_loader, val_loader


def collate_pool(dataset_list):
    batch_atom_fea, batch_nbr_fea, batch_nbr_fea_idx = [], [], []
    crystal_atom_idx, batch_target = [], []
    batch_cif_ids = []
    base_idx = 0
    for (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id in dataset_list:
        n_i = atom_fea.shape[0]
        batch_atom_fea.append(atom_fea)
        batch_nbr_fea.append(nbr_fea)
        batch_nbr_fea_idx.append(nbr_fea_idx + base_idx)
        new_idx = torch.LongTensor(np.arange(n_i) + base_idx)
        crystal_atom_idx.append(new_idx)
        batch_target.append(target)
        batch_cif_ids.append(cif_id)
        base_idx += n_i
    return (
        (
            torch.cat(batch_atom_fea, dim=0),
            torch.cat(batch_nbr_fea, dim=0),
            torch.cat(batch_nbr_fea_idx, dim=0),
            crystal_atom_idx,
        ),
        torch.stack(batch_target, dim=0),
        batch_cif_ids,
    )


class GaussianDistance(object):
    def __init__(self, dmin, dmax, step, var=None):
        assert dmin < dmax and dmax - dmin > step
        self.filter = np.arange(dmin, dmax + step, step)
        self.var = step if var is None else var

    def expand(self, distances):
        return np.exp(-((distances[..., np.newaxis] - self.filter) ** 2) / self.var**2)


class AtomInitializer(object):
    def __init__(self, atom_types):
        self.atom_types = set(atom_types)
        self._embedding = {}

    def get_atom_fea(self, atom_type):
        assert atom_type in self.atom_types
        return self._embedding[atom_type]

    def load_state_dict(self, state_dict):
        self._embedding = state_dict
        self.atom_types = set(self._embedding.keys())
        self._decodedict = {
            idx: atom_type for atom_type, idx in self._embedding.items()
        }

    def state_dict(self):
        return self._embedding

    def decode(self, idx):
        if not hasattr(self, "_decodedict"):
            self._decodedict = {
                idx: atom_type for atom_type, idx in self._embedding.items()
            }
        return self._decodedict[idx]


class AtomCustomJSONInitializer(AtomInitializer):
    def __init__(self, elem_embedding_file):
        with open(elem_embedding_file) as f:
            elem_embedding = json.load(f)
        elem_embedding = {int(key): value for key, value in elem_embedding.items()}
        super().__init__(set(elem_embedding.keys()))
        for k, v in elem_embedding.items():
            self._embedding[k] = np.array(v, dtype=float)


if CIFDATA_VARIANT == "origin":

    class CIFData(Dataset):
        def __init__(
            self,
            struct_dir,
            meta_file,
            id_col,
            target_col,
            atom_init_path,
            max_num_nbr=12,
            radius=8,
            dmin=0,
            step=0.2,
            random_seed=seed,
        ):
            self.struct_dir = struct_dir
            self.max_num_nbr, self.radius = max_num_nbr, radius
            assert os.path.exists(
                struct_dir
            ), f"struct_dir does not exist: {struct_dir}"
            assert os.path.exists(meta_file), f"meta file not found: {meta_file}"
            assert os.path.exists(
                atom_init_path
            ), f"atom_init.json not found: {atom_init_path}"
            ext = os.path.splitext(meta_file)[1].lower()
            if ext in [".csv", ".txt"]:
                df = pd.read_csv(meta_file)
            elif ext in [".xls", ".xlsx"]:
                df = pd.read_excel(meta_file)
            else:
                df = pd.read_csv(meta_file)
            assert (
                id_col in df.columns and target_col in df.columns
            ), f"Columns not found: {id_col}, {target_col}"
            df = df[[id_col, target_col]].copy()
            df[id_col] = df[id_col].astype(str)
            df[target_col] = pd.to_numeric(df[target_col], errors="coerce")
            df = df.dropna(subset=[id_col, target_col])
            pairs = []
            for _id, y in zip(df[id_col].tolist(), df[target_col].tolist()):
                path = self._find_structure_path(_id)
                if path is not None:
                    pairs.append((_id, float(y)))
            assert (
                len(pairs) > 0
            ), "No valid (id, target) with existing structure files."
            random.seed(random_seed)
            random.shuffle(pairs)
            self.id_prop_data = pairs
            self.ari = AtomCustomJSONInitializer(atom_init_path)
            self.gdf = GaussianDistance(dmin=dmin, dmax=self.radius, step=step)

        def __len__(self):
            return len(self.id_prop_data)

        def _find_structure_path(self, sid: str):
            exts = (".cif", ".CIF", ".vasp", ".VASP")
            for ext in exts:
                p = os.path.join(self.struct_dir, f"{sid}{ext}")
                if os.path.exists(p):
                    return p
            return None

        @functools.lru_cache(maxsize=None)
        def __getitem__(self, idx):
            cif_id, target = self.id_prop_data[idx]
            path = self._find_structure_path(cif_id)
            assert path is not None, f"No structure file found for {cif_id}"
            crystal = Structure.from_file(path)
            atom_fea_list = []
            for site in crystal:
                fea = None
                for sp, occu in site.species.items():
                    this_fea = np.array(
                        self.ari.get_atom_fea(sp.number), dtype=float
                    ) * float(occu)
                    fea = this_fea if fea is None else fea + this_fea
                atom_fea_list.append(fea)
            atom_fea = torch.Tensor(np.vstack(atom_fea_list))
            all_nbrs = crystal.get_all_neighbors(self.radius, include_index=True)
            all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]
            nbr_fea_idx, nbr_fea = [], []
            for nbr in all_nbrs:
                if len(nbr) < self.max_num_nbr:
                    warnings.warn(
                        f"{cif_id} not find enough neighbors to build graph. Consider increasing radius."
                    )
                    nbr_fea_idx.append(
                        [x[2] for x in nbr] + [0] * (self.max_num_nbr - len(nbr))
                    )
                    nbr_fea.append(
                        [x[1] for x in nbr]
                        + [self.radius + 1.0] * (self.max_num_nbr - len(nbr))
                    )
                else:
                    nbr_fea_idx.append([x[2] for x in nbr[: self.max_num_nbr]])
                    nbr_fea.append([x[1] for x in nbr[: self.max_num_nbr]])
            nbr_fea_idx, nbr_fea = np.array(nbr_fea_idx), np.array(nbr_fea)
            nbr_fea = self.gdf.expand(nbr_fea)
            return (
                (atom_fea, torch.Tensor(nbr_fea), torch.LongTensor(nbr_fea_idx)),
                torch.Tensor([float(target)]),
                cif_id,
            )

elif CIFDATA_VARIANT == "atom":

    class CIFData(Dataset):
        def __init__(
            self,
            struct_dir,
            meta_file,
            id_col,
            target_col,
            atom_init_path,
            max_num_nbr=12,
            radius=8,
            dmin=0,
            step=0.2,
            random_seed=seed,
            normalize_en=True,
            atom_en_dim=16,
        ):
            self.struct_dir = struct_dir
            self.max_num_nbr, self.radius = max_num_nbr, radius
            assert os.path.exists(struct_dir)
            assert os.path.exists(meta_file)
            assert os.path.exists(atom_init_path)
            ext = os.path.splitext(meta_file)[1].lower()
            if ext in [".csv", ".txt"]:
                df = pd.read_csv(meta_file)
            elif ext in [".xls", ".xlsx"]:
                df = pd.read_excel(meta_file)
            else:
                df = pd.read_csv(meta_file)
            assert id_col in df.columns and target_col in df.columns
            df = df[[id_col, target_col]].copy()
            df[id_col] = df[id_col].astype(str)
            df[target_col] = pd.to_numeric(df[target_col], errors="coerce")
            df = df.dropna(subset=[id_col, target_col])
            pairs = []
            for _id, y in zip(df[id_col].tolist(), df[target_col].tolist()):
                cif_path = os.path.join(self.struct_dir, f"{_id}.cif")
                if os.path.exists(cif_path):
                    pairs.append((_id, float(y)))
            assert len(pairs) > 0
            random.seed(random_seed)
            random.shuffle(pairs)
            self.id_prop_data = pairs
            self.ari = AtomCustomJSONInitializer(atom_init_path)
            self.gdf = GaussianDistance(dmin=dmin, dmax=self.radius, step=step)
            en = {
                "H": 2.20,
                "Li": 0.98,
                "Be": 1.57,
                "B": 2.04,
                "C": 2.55,
                "N": 3.04,
                "O": 3.44,
                "F": 3.98,
                "Na": 0.93,
                "Mg": 1.31,
                "Al": 1.61,
                "Si": 1.90,
                "P": 2.19,
                "S": 2.58,
                "Cl": 3.16,
                "K": 0.82,
                "Ca": 1.00,
                "Sc": 1.36,
                "Ti": 1.54,
                "V": 1.63,
                "Cr": 1.66,
                "Mn": 1.55,
                "Fe": 1.83,
                "Co": 1.88,
                "Ni": 1.91,
                "Cu": 1.90,
                "Zn": 1.65,
                "Ga": 1.81,
                "Ge": 2.01,
                "As": 2.18,
                "Se": 2.55,
                "Br": 2.96,
                "Kr": 3.00,
                "Rb": 0.82,
                "Sr": 0.95,
                "Y": 1.22,
                "Zr": 1.33,
                "Nb": 1.60,
                "Mo": 2.16,
                "Tc": 1.90,
                "Ru": 2.20,
                "Rh": 2.28,
                "Pd": 2.20,
                "Ag": 1.93,
                "Cd": 1.69,
                "In": 1.78,
                "Sn": 1.96,
                "Sb": 2.05,
                "Te": 2.10,
                "I": 2.66,
                "Xe": 2.60,
                "Cs": 0.79,
                "Ba": 0.89,
                "La": 1.10,
                "Ce": 1.12,
                "Pr": 1.13,
                "Nd": 1.14,
                "Pm": 1.13,
                "Sm": 1.17,
                "Eu": 1.20,
                "Gd": 1.20,
                "Tb": 1.22,
                "Dy": 1.23,
                "Ho": 1.24,
                "Er": 1.24,
                "Tm": 1.25,
                "Yb": 1.10,
                "Lu": 1.27,
                "Hf": 1.30,
                "Ta": 1.50,
                "W": 2.36,
                "Re": 1.90,
                "Os": 2.20,
                "Ir": 2.20,
                "Pt": 2.28,
                "Au": 2.54,
                "Hg": 2.00,
                "Tl": 1.62,
                "Pb": 2.33,
                "Bi": 2.02,
            }
            self.en_table = en
            all_vals = np.array(list(self.en_table.values()), dtype=float)
            self.en_default = float(all_vals.mean())
            self.normalize_en = normalize_en
            if self.normalize_en:
                self.en_min, self.en_max = float(all_vals.min()), float(all_vals.max())
                self.en_scale = (
                    self.en_max - self.en_min if self.en_max > self.en_min else 1.0
                )
            self.en_proj_weight = torch.randn(1, atom_en_dim) * 0.1
            self.en_proj_bias = torch.zeros(atom_en_dim)

        def __len__(self):
            return len(self.id_prop_data)

        def _get_en_scalar(self, specie):
            symbol = getattr(specie, "symbol", str(specie))
            en = self.en_table.get(symbol, self.en_default)
            if self.normalize_en:
                en = (en - self.en_min) / self.en_scale
            return float(en)

        @functools.lru_cache(maxsize=None)
        def __getitem__(self, idx):
            cif_id, target = self.id_prop_data[idx]
            crystal = Structure.from_file(
                os.path.join(self.struct_dir, cif_id + ".cif")
            )
            atom_fea_list = []
            atom_en_list = []
            for site in crystal:
                fea = None
                en_val = 0.0
                for sp, occu in site.species.items():
                    base = np.array(
                        self.ari.get_atom_fea(sp.number), dtype=float
                    ) * float(occu)
                    fea = base if fea is None else fea + base
                    en_val += self._get_en_scalar(sp) * float(occu)
                atom_fea_list.append(fea)
                atom_en_list.append([en_val])
            atom_fea_np = np.vstack(atom_fea_list)
            atom_en_np = np.array(atom_en_list, dtype=float)
            atom_fea = torch.from_numpy(atom_fea_np).float()
            with torch.no_grad():
                en_emb = (
                    atom_en_np @ self.en_proj_weight.numpy() + self.en_proj_bias.numpy()
                )
            en_emb = torch.from_numpy(en_emb).float()
            atom_fea = torch.cat([atom_fea, en_emb], dim=1)
            all_nbrs = crystal.get_all_neighbors(self.radius, include_index=True)
            all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]
            nbr_fea_idx, nbr_fea = [], []
            for nbr in all_nbrs:
                if len(nbr) < self.max_num_nbr:
                    warnings.warn(f"{cif_id} not enough neighbors.")
                    nbr_fea_idx.append(
                        [x[2] for x in nbr] + [0] * (self.max_num_nbr - len(nbr))
                    )
                    nbr_fea.append(
                        [x[1] for x in nbr]
                        + [self.radius + 1.0] * (self.max_num_nbr - len(nbr))
                    )
                else:
                    nbr_fea_idx.append([x[2] for x in nbr[: self.max_num_nbr]])
                    nbr_fea.append([x[1] for x in nbr[: self.max_num_nbr]])
            nbr_fea_idx = np.array(nbr_fea_idx)
            nbr_fea = self.gdf.expand(np.array(nbr_fea))
            return (
                (atom_fea, torch.Tensor(nbr_fea), torch.LongTensor(nbr_fea_idx)),
                torch.Tensor([float(target)]),
                cif_id,
            )

elif CIFDATA_VARIANT == "edge":

    class CIFData(Dataset):
        def __init__(
            self,
            struct_dir,
            meta_file,
            id_col,
            target_col,
            atom_init_path,
            max_num_nbr=12,
            radius=8,
            dmin=0,
            step=0.2,
            random_seed=seed,
            normalize_en=True,
            edge_en_dim=8,
        ):
            self.struct_dir = struct_dir
            self.max_num_nbr, self.radius = max_num_nbr, radius
            assert os.path.exists(struct_dir)
            assert os.path.exists(meta_file)
            assert os.path.exists(atom_init_path)
            ext = os.path.splitext(meta_file)[1].lower()
            if ext in [".csv", ".txt"]:
                df = pd.read_csv(meta_file)
            elif ext in [".xls", ".xlsx"]:
                df = pd.read_excel(meta_file)
            else:
                df = pd.read_csv(meta_file)
            assert id_col in df.columns and target_col in df.columns
            df = df[[id_col, target_col]].copy()
            df[id_col] = df[id_col].astype(str)
            df[target_col] = pd.to_numeric(df[target_col], errors="coerce")
            df = df.dropna(subset=[id_col, target_col])
            pairs = []
            for _id, y in zip(df[id_col].tolist(), df[target_col].tolist()):
                cif_path = os.path.join(self.struct_dir, f"{_id}.cif")
                if os.path.exists(cif_path):
                    pairs.append((_id, float(y)))
            assert len(pairs) > 0
            random.seed(random_seed)
            random.shuffle(pairs)
            self.id_prop_data = pairs
            self.ari = AtomCustomJSONInitializer(atom_init_path)
            self.gdf = GaussianDistance(dmin=dmin, dmax=self.radius, step=step)
            en = {
                "H": 2.20,
                "Li": 0.98,
                "Be": 1.57,
                "B": 2.04,
                "C": 2.55,
                "N": 3.04,
                "O": 3.44,
                "F": 3.98,
                "Na": 0.93,
                "Mg": 1.31,
                "Al": 1.61,
                "Si": 1.90,
                "P": 2.19,
                "S": 2.58,
                "Cl": 3.16,
                "K": 0.82,
                "Ca": 1.00,
                "Sc": 1.36,
                "Ti": 1.54,
                "V": 1.63,
                "Cr": 1.66,
                "Mn": 1.55,
                "Fe": 1.83,
                "Co": 1.88,
                "Ni": 1.91,
                "Cu": 1.90,
                "Zn": 1.65,
                "Ga": 1.81,
                "Ge": 2.01,
                "As": 2.18,
                "Se": 2.55,
                "Br": 2.96,
                "Kr": 3.00,
                "Rb": 0.82,
                "Sr": 0.95,
                "Y": 1.22,
                "Zr": 1.33,
                "Nb": 1.60,
                "Mo": 2.16,
                "Tc": 1.90,
                "Ru": 2.20,
                "Rh": 2.28,
                "Pd": 2.20,
                "Ag": 1.93,
                "Cd": 1.69,
                "In": 1.78,
                "Sn": 1.96,
                "Sb": 2.05,
                "Te": 2.10,
                "I": 2.66,
                "Xe": 2.60,
                "Cs": 0.79,
                "Ba": 0.89,
                "La": 1.10,
                "Ce": 1.12,
                "Pr": 1.13,
                "Nd": 1.14,
                "Pm": 1.13,
                "Sm": 1.17,
                "Eu": 1.20,
                "Gd": 1.20,
                "Tb": 1.22,
                "Dy": 1.23,
                "Ho": 1.24,
                "Er": 1.24,
                "Tm": 1.25,
                "Yb": 1.10,
                "Lu": 1.27,
                "Hf": 1.30,
                "Ta": 1.50,
                "W": 2.36,
                "Re": 1.90,
                "Os": 2.20,
                "Ir": 2.20,
                "Pt": 2.28,
                "Au": 2.54,
                "Hg": 2.00,
                "Tl": 1.62,
                "Pb": 2.33,
                "Bi": 2.02,
            }
            self.en_table = en
            all_vals = np.array(list(self.en_table.values()), dtype=float)
            self.en_default = float(all_vals.mean())
            self.normalize_en = normalize_en
            if self.normalize_en:
                self.en_min, self.en_max = float(all_vals.min()), float(all_vals.max())
                self.en_scale = (
                    self.en_max - self.en_min if self.en_max > self.en_min else 1.0
                )
            mode = str(DELTA_EN_FEAT_MODE).lower()
            if mode == "raw":
                self.delta_feat_dim = 1
            elif mode == "poly":
                self.delta_feat_dim = 2
            elif mode == "rbf":
                self.delta_feat_dim = 1
            elif mode == "fourier":
                self.delta_feat_dim = 2 * int(DELTA_EN_FOURIER_K)
            elif mode == "all":
                self.delta_feat_dim = 2 + 1 + 2 * int(DELTA_EN_FOURIER_K)
            else:
                self.delta_feat_dim = 2
            self.edge_proj_weight = torch.randn(self.delta_feat_dim, edge_en_dim) * 0.1
            self.edge_proj_bias = torch.zeros(edge_en_dim)

        def __len__(self):
            return len(self.id_prop_data)

        def _get_en(self, specie):
            symbol = getattr(specie, "symbol", str(specie))
            en = self.en_table.get(symbol, self.en_default)
            if self.normalize_en:
                en = (en - self.en_min) / self.en_scale
            return float(en)

        def _delta_features(self, delta):
            mode = str(DELTA_EN_FEAT_MODE).lower()
            if mode == "raw":
                feats = [delta]
            elif mode == "poly":
                feats = [delta, delta**2]
            elif mode == "rbf":
                gamma = float(DELTA_EN_RBF_GAMMA)
                feats = [np.exp(-gamma * (delta**2))]
            elif mode == "fourier":
                K = int(DELTA_EN_FOURIER_K)
                feats = []
                for k in range(1, K + 1):
                    feats.append(np.sin(k * delta))
                    feats.append(np.cos(k * delta))
            elif mode == "all":
                K = int(DELTA_EN_FOURIER_K)
                gamma = float(DELTA_EN_RBF_GAMMA)
                feats = [delta, delta**2, np.exp(-gamma * (delta**2))]
                for k in range(1, K + 1):
                    feats.append(np.sin(k * delta))
                    feats.append(np.cos(k * delta))
            else:
                feats = [delta, delta**2]
            return np.concatenate(feats, axis=-1)

        @functools.lru_cache(maxsize=None)
        def __getitem__(self, idx):
            cif_id, target = self.id_prop_data[idx]
            crystal = Structure.from_file(
                os.path.join(self.struct_dir, cif_id + ".cif")
            )
            atom_fea_list = []
            en_vec = []
            for site in crystal:
                fea = None
                en_val = 0.0
                for sp, occu in site.species.items():
                    base = np.array(
                        self.ari.get_atom_fea(sp.number), dtype=float
                    ) * float(occu)
                    fea = base if fea is None else fea + base
                    en_val += self._get_en(sp) * float(occu)
                atom_fea_list.append(fea)
                en_vec.append(en_val)
            atom_fea = torch.from_numpy(np.vstack(atom_fea_list)).float()
            en_vec = np.asarray(en_vec, dtype=float)
            all_nbrs = crystal.get_all_neighbors(self.radius, include_index=True)
            all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]
            nbr_fea_idx, nbr_dists = [], []
            for nbr in all_nbrs:
                if len(nbr) < self.max_num_nbr:
                    warnings.warn(f"{cif_id} not enough neighbors.")
                    nbr_fea_idx.append(
                        [x[2] for x in nbr] + [0] * (self.max_num_nbr - len(nbr))
                    )
                    nbr_dists.append(
                        [x[1] for x in nbr]
                        + [self.radius + 1.0] * (self.max_num_nbr - len(nbr))
                    )
                else:
                    nbr_fea_idx.append([x[2] for x in nbr[: self.max_num_nbr]])
                    nbr_dists.append([x[1] for x in nbr[: self.max_num_nbr]])
            nbr_fea_idx = np.asarray(nbr_fea_idx, dtype=int)
            nbr_dists = np.asarray(nbr_dists, dtype=float)
            nbr_fea = self.gdf.expand(nbr_dists)
            delta_en = np.abs(en_vec[:, None] - en_vec[nbr_fea_idx])[..., None]
            delta_feats = self._delta_features(delta_en)
            with torch.no_grad():
                delta_emb = (
                    delta_feats @ self.edge_proj_weight.numpy()
                    + self.edge_proj_bias.numpy()
                )
            delta_emb = torch.from_numpy(delta_emb).float()
            nbr_fea = torch.cat([torch.from_numpy(nbr_fea).float(), delta_emb], dim=2)
            return (
                (atom_fea, nbr_fea, torch.LongTensor(nbr_fea_idx)),
                torch.tensor([float(target)], dtype=torch.float32),
                cif_id,
            )


# ----------------------------- Model -----------------------------
class ConvLayer(nn.Module):
    def __init__(self, atom_fea_len, nbr_fea_len):
        super().__init__()
        self.atom_fea_len = atom_fea_len
        self.fc_full = nn.Linear(2 * atom_fea_len + nbr_fea_len, 2 * atom_fea_len)
        self.sigmoid = nn.Sigmoid()
        self.softplus1 = nn.Softplus()
        self.bn1 = nn.BatchNorm1d(2 * atom_fea_len)
        self.bn2 = nn.BatchNorm1d(atom_fea_len)
        self.softplus2 = nn.Softplus()

    def forward(self, atom_in_fea, nbr_fea, nbr_fea_idx):
        N, M = nbr_fea_idx.shape
        atom_nbr_fea = atom_in_fea[nbr_fea_idx, :]
        total_nbr_fea = torch.cat(
            [
                atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len),
                atom_nbr_fea,
                nbr_fea,
            ],
            dim=2,
        )
        total_gated_fea = self.fc_full(total_nbr_fea)
        total_gated_fea = self.bn1(
            total_gated_fea.view(-1, self.atom_fea_len * 2)
        ).view(N, M, self.atom_fea_len * 2)
        nbr_filter, nbr_core = total_gated_fea.chunk(2, dim=2)
        nbr_sumed = torch.sum(
            self.sigmoid(nbr_filter) * self.softplus1(nbr_core), dim=1
        )
        nbr_sumed = self.bn2(nbr_sumed)
        return self.softplus2(atom_in_fea + nbr_sumed)


class CrystalGraphConvNet(nn.Module):
    def __init__(
        self,
        orig_atom_fea_len,
        nbr_fea_len,
        atom_fea_len=64,
        n_conv=3,
        h_fea_len=128,
        n_h=1,
    ):
        super().__init__()
        self.embedding = nn.Linear(orig_atom_fea_len, atom_fea_len)
        self.convs = nn.ModuleList(
            [
                ConvLayer(atom_fea_len=atom_fea_len, nbr_fea_len=nbr_fea_len)
                for _ in range(n_conv)
            ]
        )
        self.conv_to_fc = nn.Linear(atom_fea_len, h_fea_len)
        self.softplus = nn.Softplus()
        if n_h > 1:
            self.fcs = nn.ModuleList(
                [nn.Linear(h_fea_len, h_fea_len) for _ in range(n_h - 1)]
            )
            self.softpluses = nn.ModuleList([nn.Softplus() for _ in range(n_h - 1)])
        self.fc_out = nn.Linear(h_fea_len, 1)

    def pooling(self, atom_fea, crystal_atom_idx):
        assert (
            sum([len(idx_map) for idx_map in crystal_atom_idx])
            == atom_fea.data.shape[0]
        )
        return torch.cat(
            [
                torch.mean(atom_fea[idx_map], dim=0, keepdim=True)
                for idx_map in crystal_atom_idx
            ],
            dim=0,
        )

    def forward(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx):
        atom_fea = self.embedding(atom_fea)
        for conv in self.convs:
            atom_fea = conv(atom_fea, nbr_fea, nbr_fea_idx)
        crys_fea = self.pooling(atom_fea, crystal_atom_idx)
        crys_fea = self.softplus(self.conv_to_fc(self.softplus(crys_fea)))
        if hasattr(self, "fcs"):
            for fc, sp in zip(self.fcs, self.softpluses):
                crys_fea = sp(fc(crys_fea))
        return self.fc_out(crys_fea)


# ----------------------------- Training helpers -----------------------------
if n_folds == "none":

    class Normalizer(object):
        def __init__(self, tensor):
            self.mean = torch.mean(tensor)
            s = torch.std(tensor)
            self.std = s if float(s) > 0 else torch.tensor(1.0)

        def norm(self, tensor):
            return (tensor - self.mean) / self.std

        def denorm(self, normed_tensor):
            return normed_tensor * self.std + self.mean

        def state_dict(self):
            return {"mean": self.mean, "std": self.std}

        def load_state_dict(self, state_dict):
            self.mean = state_dict["mean"]
            self.std = (
                state_dict["std"]
                if float(state_dict["std"]) != 0
                else torch.tensor(1.0)
            )

else:

    class Normalizer(object):
        def __init__(self, tensor):
            self.mean = torch.mean(tensor)
            s = torch.std(tensor)
            self.std = s if float(s) > 0 else torch.tensor(1.0)

        def norm(self, tensor):
            mean = self.mean.to(tensor.device)
            std = (
                self.std.to(tensor.device)
                if isinstance(self.std, torch.Tensor)
                else torch.tensor(self.std, device=tensor.device)
            )
            return (tensor - mean) / std

        def denorm(self, normed_tensor):
            mean = self.mean.to(normed_tensor.device)
            std = (
                self.std.to(normed_tensor.device)
                if isinstance(self.std, torch.Tensor)
                else torch.tensor(self.std, device=normed_tensor.device)
            )
            return normed_tensor * std + mean

        def state_dict(self):
            return {"mean": self.mean, "std": self.std}

        def load_state_dict(self, state_dict):
            self.mean = state_dict["mean"]
            self.std = (
                state_dict["std"]
                if float(state_dict["std"]) != 0
                else torch.tensor(1.0)
            )


def save_checkpoint(state, is_best, ckpt_path, best_path):
    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)
    torch.save(state, ckpt_path)
    if is_best:
        import shutil

        shutil.copyfile(ckpt_path, best_path)


class EarlyStopping:
    def __init__(self, patience=patience, min_delta=0.0):
        self.patience = patience
        self.min_delta = min_delta
        self.best = float("inf")
        self.num_bad = 0
        self.stopped = False

    def step(self, value):
        improved = (self.best - value) > self.min_delta
        if improved:
            self.best = value
            self.num_bad = 0
        else:
            self.num_bad += 1
            if self.num_bad >= self.patience:
                self.stopped = True
        return improved


# ----------------------------- Config -----------------------------
@dataclass
class Config:
    struct_dir: str = STRUCT_DIR
    atom_init_path: str = ATOM_INIT_PATH
    meta_file: str = META_FILE
    id_col: str = ID_COL
    target_col: str = TARGET_COL
    train_ratio: float = train_ratio
    val_ratio: float = val_ratio
    test_ratio: float = test_ratio
    train_size: Optional[int] = None
    val_size: Optional[int] = None
    test_size: Optional[int] = None
    workers: int = 0
    epochs: int = epochs
    batch_size: int = batch_size
    lr: float = lr
    lr_milestones: List[int] = None
    optim: str = "Adam"
    atom_fea_len: int = 64
    h_fea_len: int = 128
    n_conv: int = 3
    n_h: int = 1
    print_freq: int = print_freq
    use_cuda: bool = True
    early_stop_patience: int = 10
    out_dir: str = os.path.join(SCRIPT_DIR, "outputs")

    def __post_init__(self):
        if self.lr_milestones is None:
            self.lr_milestones = [100]


# ----------------------------- Core loop -----------------------------
def train_one_epoch(
    loader, model, criterion, optimizer, epoch, normalizer, device, print_freq
):
    model.train()
    sq_sum = 0.0
    n_total = 0
    for i, (inp, target, _) in enumerate(loader):
        a0 = inp[0].to(device, non_blocking=True)
        a1 = inp[1].to(device, non_blocking=True)
        a2 = inp[2].to(device, non_blocking=True)
        a3 = [idx.to(device, non_blocking=True) for idx in inp[3]]
        target_normed = normalizer.norm(target).to(device, non_blocking=True)
        output = model(a0, a1, a2, a3)
        loss = criterion(output, target_normed)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        with torch.no_grad():
            pred = normalizer.denorm(output.detach().cpu()).view(-1)
            true = target.view(-1).cpu()
            diff = true - pred
            sq_sum += torch.sum(diff * diff).item()
            n_total += true.numel()
        if i % print_freq == 0:
            rmse_now = np.sqrt(sq_sum / max(1, n_total))
            print(f"Epoch: [{epoch:>3}][{i:>2}/{len(loader):<2}] RMSE {rmse_now:8.6f}")
    return float(np.sqrt(sq_sum / max(1, n_total)))


def evaluate(loader, model, normalizer, device, collect=False, out_csv=None):
    model.eval()
    y_true_all, y_pred_all, ids_all = [], [], []
    with torch.no_grad():
        for inp, target, batch_ids in loader:
            a0 = inp[0].to(device, non_blocking=True)
            a1 = inp[1].to(device, non_blocking=True)
            a2 = inp[2].to(device, non_blocking=True)
            a3 = [idx.to(device, non_blocking=True) for idx in inp[3]]
            output = model(a0, a1, a2, a3)
            pred = normalizer.denorm(output.detach().cpu()).view(-1).numpy()
            true = target.view(-1).cpu().numpy()
            y_true_all.append(true)
            y_pred_all.append(pred)
            if collect:
                ids_all.extend(batch_ids)
    y_true = np.concatenate(y_true_all, axis=0) if y_true_all else np.array([])
    y_pred = np.concatenate(y_pred_all, axis=0) if y_pred_all else np.array([])
    mae = float(np.mean(np.abs(y_true - y_pred))) if y_true.size else float("nan")
    rmse = (
        float(np.sqrt(np.mean((y_true - y_pred) ** 2))) if y_true.size else float("nan")
    )
    r2 = (
        float(r2_score(y_true, y_pred))
        if (y_true.size and np.var(y_true) > 0)
        else float("nan")
    )
    if collect and out_csv is not None:
        with open(out_csv, "w", newline="") as f:
            w = csv.writer(f, lineterminator="\n")
            w.writerow(["id", "true", "predict"])
            for cid, t, p in zip(ids_all, y_true.tolist(), y_pred.tolist()):
                w.writerow([cid, t, p])
    return {"mae": mae, "rmse": rmse, "r2": r2}


if n_folds == "none":

    def run_experiment(cfg: Config, trial_tag: Optional[str] = None):
        t0 = time.time()
        device = torch.device(
            "cuda" if (cfg.use_cuda and torch.cuda.is_available()) else "cpu"
        )
        os.makedirs(cfg.out_dir, exist_ok=True)
        loss_path = os.path.join(cfg.out_dir, "loss.dat")
        ckpt_path = os.path.join(cfg.out_dir, "checkpoint.pth.tar")
        best_path = os.path.join(cfg.out_dir, "model_best.pth.tar")
        train_csv = os.path.join(cfg.out_dir, "train_results.csv")
        val_csv = os.path.join(cfg.out_dir, "vali_results.csv")
        test_csv = os.path.join(cfg.out_dir, "test_results.csv")
        log_path = os.path.join(cfg.out_dir, "log.dat")
        dataset = CIFData(
            cfg.struct_dir,
            cfg.meta_file,
            cfg.id_col,
            cfg.target_col,
            cfg.atom_init_path,
        )
        train_loader, val_loader, test_loader = get_train_val_test_loader(
            dataset=dataset,
            collate_fn=collate_pool,
            batch_size=min(cfg.batch_size, len(dataset)),
            train_ratio=cfg.train_ratio,
            val_ratio=cfg.val_ratio,
            test_ratio=cfg.test_ratio,
            num_workers=cfg.workers,
            pin_memory=(device.type == "cuda"),
            train_size=cdf(cfg.train_size),
            val_size=cdf(cfg.val_size),
            test_size=cdf(cfg.test_size),
            return_test=True,
        )
        if len(dataset) < 500:
            warnings.warn(
                "Dataset has less than 500 data points. Lower accuracy is expected."
            )
            sample_data_list = [dataset[i] for i in range(len(dataset))]
        else:
            from random import sample as rsample

            sample_data_list = [dataset[i] for i in rsample(range(len(dataset)), 500)]
        _, sample_target, _ = collate_pool(sample_data_list)
        normalizer = Normalizer(sample_target)
        structures, _, _ = dataset[0]
        orig_atom_fea_len = structures[0].shape[-1]
        nbr_fea_len = structures[1].shape[-1]
        model = CrystalGraphConvNet(
            orig_atom_fea_len,
            nbr_fea_len,
            atom_fea_len=cfg.atom_fea_len,
            n_conv=cfg.n_conv,
            h_fea_len=cfg.h_fea_len,
            n_h=cfg.n_h,
        ).to(device)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), cfg.lr)
        # scheduler = MultiStepLR(optimizer, milestones=list(cfg.lr_milestones), gamma=0.3)
        scheduler = StepLR(optimizer, step_size=50, gamma=0.5)
        es = EarlyStopping(patience=cfg.early_stop_patience)
        best_val_rmse = float("inf")
        with open(loss_path, "w", newline="") as lf:
            lf.write("epoch train_rmse val_rmse\n")
            for epoch in range(cfg.epochs):
                train_rmse = train_one_epoch(
                    train_loader,
                    model,
                    criterion,
                    optimizer,
                    epoch,
                    normalizer,
                    device,
                    cfg.print_freq,
                )
                val_metrics = evaluate(
                    val_loader, model, normalizer, device, collect=False
                )
                val_rmse = val_metrics["rmse"]
                if np.isnan(val_rmse):
                    print("Exit due to NaN")
                    break
                scheduler.step()
                lf.write(f"{epoch+1} {train_rmse:.6f} {val_rmse:.6f}\n")
                lf.flush()
                is_best = val_rmse < best_val_rmse
                best_val_rmse = min(val_rmse, best_val_rmse)
                save_checkpoint(
                    {
                        "epoch": epoch + 1,
                        "state_dict": model.state_dict(),
                        "best_val_rmse": best_val_rmse,
                        "optimizer": optimizer.state_dict(),
                        "normalizer": normalizer.state_dict(),
                        "config": asdict(cfg),
                    },
                    is_best,
                    ckpt_path,
                    best_path,
                )
                es.step(val_rmse)
                if es.stopped:
                    print(f"Early stopped at epoch {epoch+1}")
                    break
        print("---------Evaluate Best Model on Train/Val/Test---------------")
        best_ckpt = torch.load(best_path, map_location=device)
        model.load_state_dict(best_ckpt["state_dict"])
        train_metrics = evaluate(
            train_loader, model, normalizer, device, collect=True, out_csv=train_csv
        )
        val_metrics = evaluate(
            val_loader, model, normalizer, device, collect=True, out_csv=val_csv
        )
        test_metrics = evaluate(
            test_loader, model, normalizer, device, collect=True, out_csv=test_csv
        )
        with open(log_path, "w", encoding="utf-8") as logf:
            model_str = str(model)
            print(model_str)
            logf.write("===== Model Architecture =====\n")
            logf.write(model_str + "\n")
            n_params = sum(p.numel() for p in model.parameters())
            n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
            logf.write(f"Total params: {n_params}\n")
            logf.write(f"Trainable params: {n_trainable}\n")
            logf.write("==============================\n")

            def line(name, m):
                s = f"{name}: MAE={m['mae']:.6f} RMSE={m['rmse']:.6f} R2={m['r2']:.6f}"
                print(s)
                logf.write(s + "\n")

            line("Train", train_metrics)
            line("Val", val_metrics)
            line("Test", test_metrics)
            runtime = time.time() - t0
            logf.write(f"Runtime(s): {runtime:.3f}\n")
            print(f"Runtime(s): {runtime:.3f}")
        return {
            "best_val_rmse": best_val_rmse,
            "train": train_metrics,
            "val": val_metrics,
            "test": test_metrics,
        }

else:

    def run_experiment(cfg: Config, trial_tag: Optional[str] = None):
        t0 = time.time()
        device = torch.device(
            "cuda" if (cfg.use_cuda and torch.cuda.is_available()) else "cpu"
        )
        os.makedirs(cfg.out_dir, exist_ok=True)
        dataset = CIFData(
            cfg.struct_dir,
            cfg.meta_file,
            cfg.id_col,
            cfg.target_col,
            cfg.atom_init_path,
        )
        train_loader_all, _val_ignore, test_loader = get_train_val_test_loader(
            dataset=dataset,
            collate_fn=collate_pool,
            batch_size=min(cfg.batch_size, len(dataset)),
            train_ratio=cfg.train_ratio,
            val_ratio=0.0,
            test_ratio=cfg.test_ratio,
            num_workers=cfg.workers,
            pin_memory=(device.type == "cuda"),
            train_size=cdf(cfg.train_size),
            val_size=None,
            test_size=cdf(cfg.test_size),
            return_test=True,
        )
        base_train_indices = list(train_loader_all.sampler.indices)
        structures, _, _ = dataset[0]
        orig_atom_fea_len = structures[0].shape[-1]
        nbr_fea_len = structures[1].shape[-1]
        cv_summary_path = os.path.join(cfg.out_dir, "cv_summary.dat")
        with open(cv_summary_path, "w", encoding="utf-8") as cvf:
            cvf.write("fold best_val_rmse\n")
        kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)
        best_overall = {
            "fold": -1,
            "best_val_rmse": float("inf"),
            "best_path": None,
            "train_idx": None,
            "val_idx": None,
        }
        for fold_id, (train_pos, val_pos) in enumerate(
            kf.split(base_train_indices), start=1
        ):
            fold_dir = os.path.join(cfg.out_dir, f"fold_{fold_id}")
            os.makedirs(fold_dir, exist_ok=True)
            loss_path = os.path.join(fold_dir, "loss.dat")
            ckpt_path = os.path.join(fold_dir, "checkpoint.pth.tar")
            best_path = os.path.join(fold_dir, "model_best.pth.tar")
            fold_train_idx = [base_train_indices[i] for i in train_pos]
            fold_val_idx = [base_train_indices[i] for i in val_pos]
            fold_train_loader = DataLoader(
                dataset,
                batch_size=min(cfg.batch_size, max(1, len(fold_train_idx))),
                sampler=SubsetRandomSampler(fold_train_idx),
                num_workers=cfg.workers,
                collate_fn=collate_pool,
                pin_memory=(device.type == "cuda"),
            )
            fold_val_loader = DataLoader(
                dataset,
                batch_size=min(cfg.batch_size, max(1, len(fold_val_idx))),
                sampler=SubsetRandomSampler(fold_val_idx),
                num_workers=cfg.workers,
                collate_fn=collate_pool,
                pin_memory=(device.type == "cuda"),
            )
            if len(fold_train_idx) <= 500:
                norm_indices = fold_train_idx
            else:
                from random import sample as rsample

                norm_indices = rsample(fold_train_idx, 500)
            sample_data_list = [dataset[i] for i in norm_indices]
            _, sample_target, _ = collate_pool(sample_data_list)
            normalizer = Normalizer(sample_target)
            model = CrystalGraphConvNet(
                orig_atom_fea_len,
                nbr_fea_len,
                atom_fea_len=cfg.atom_fea_len,
                n_conv=cfg.n_conv,
                h_fea_len=cfg.h_fea_len,
                n_h=cfg.n_h,
            ).to(device)
            criterion = nn.MSELoss()
            optimizer = optim.Adam(model.parameters(), cfg.lr)
            # scheduler = MultiStepLR(optimizer, milestones=list(cfg.lr_milestones), gamma=0.3)
            scheduler = StepLR(optimizer, step_size=50, gamma=0.5)
            es = EarlyStopping(patience=cfg.early_stop_patience)
            best_val_rmse = float("inf")
            with open(loss_path, "w", newline="") as lf:
                lf.write("epoch train_rmse val_rmse\n")
                for epoch in range(cfg.epochs):
                    train_rmse = train_one_epoch(
                        fold_train_loader,
                        model,
                        criterion,
                        optimizer,
                        epoch,
                        normalizer,
                        device,
                        cfg.print_freq,
                    )
                    val_metrics = evaluate(
                        fold_val_loader, model, normalizer, device, collect=False
                    )
                    val_rmse = val_metrics["rmse"]
                    if np.isnan(val_rmse):
                        print(f"[Fold {fold_id}] Exit due to NaN")
                        break
                    scheduler.step()
                    lf.write(f"{epoch+1} {train_rmse:.6f} {val_rmse:.6f}\n")
                    lf.flush()
                    is_best = val_rmse < best_val_rmse
                    best_val_rmse = min(val_rmse, best_val_rmse)
                    save_checkpoint(
                        {
                            "epoch": epoch + 1,
                            "state_dict": model.state_dict(),
                            "best_val_rmse": best_val_rmse,
                            "optimizer": optimizer.state_dict(),
                            "normalizer": normalizer.state_dict(),
                            "config": asdict(cfg),
                        },
                        is_best,
                        ckpt_path,
                        best_path,
                    )
                    es.step(val_rmse)
                    if es.stopped:
                        print(f"[Fold {fold_id}] Early stopped at epoch {epoch+1}")
                        break
            with open(cv_summary_path, "a", encoding="utf-8") as cvf:
                cvf.write(f"{fold_id} {best_val_rmse:.6f}\n")
            best_ckpt = torch.load(best_path, map_location=device)
            dummy = torch.tensor([0.0])
            normalizer = Normalizer(dummy)
            normalizer.load_state_dict(best_ckpt["normalizer"])
            model.load_state_dict(best_ckpt["state_dict"])
            fold_train_loader = DataLoader(
                dataset,
                batch_size=min(cfg.batch_size, max(1, len(fold_train_idx))),
                sampler=SubsetRandomSampler(fold_train_idx),
                num_workers=cfg.workers,
                collate_fn=collate_pool,
                pin_memory=(device.type == "cuda"),
            )
            fold_val_loader = DataLoader(
                dataset,
                batch_size=min(cfg.batch_size, max(1, len(fold_val_idx))),
                sampler=SubsetRandomSampler(fold_val_idx),
                num_workers=cfg.workers,
                collate_fn=collate_pool,
                pin_memory=(device.type == "cuda"),
            )
            fold_test_loader = test_loader
            train_metrics = evaluate(
                fold_train_loader,
                model,
                normalizer,
                device,
                collect=True,
                out_csv=os.path.join(fold_dir, "scatter_train.dat"),
            )
            val_metrics = evaluate(
                fold_val_loader,
                model,
                normalizer,
                device,
                collect=True,
                out_csv=os.path.join(fold_dir, "scatter_vali.dat"),
            )
            test_metrics = evaluate(
                fold_test_loader,
                model,
                normalizer,
                device,
                collect=True,
                out_csv=os.path.join(fold_dir, "scatter_test.dat"),
            )
            log_path = os.path.join(fold_dir, "log.dat")
            with open(log_path, "w", encoding="utf-8") as flog:
                flog.write(
                    f"Train: MAE={train_metrics['mae']:.6f} RMSE={train_metrics['rmse']:.6f} R2={train_metrics['r2']:.6f}\n"
                )
                flog.write(
                    f"Val:   MAE={val_metrics['mae']:.6f} RMSE={val_metrics['rmse']:.6f} R2={val_metrics['r2']:.6f}\n"
                )
                flog.write(
                    f"Test:  MAE={test_metrics['mae']:.6f} RMSE={test_metrics['rmse']:.6f} R2={test_metrics['r2']:.6f}\n"
                )
            if best_val_rmse < best_overall["best_val_rmse"]:
                best_overall.update(
                    {
                        "fold": fold_id,
                        "best_val_rmse": best_val_rmse,
                        "best_path": best_path,
                        "train_idx": fold_train_idx,
                        "val_idx": fold_val_idx,
                    }
                )
        print("---------Evaluate Best Fold Model on Train/Val/Test---------------")
        assert best_overall["best_path"] is not None, "No best fold found."
        best_ckpt = torch.load(best_overall["best_path"], map_location=device)
        best_train_loader = DataLoader(
            dataset,
            batch_size=min(cfg.batch_size, max(1, len(best_overall["train_idx"]))),
            sampler=SubsetRandomSampler(best_overall["train_idx"]),
            num_workers=cfg.workers,
            collate_fn=collate_pool,
            pin_memory=(device.type == "cuda"),
        )
        best_val_loader = DataLoader(
            dataset,
            batch_size=min(cfg.batch_size, max(1, len(best_overall["val_idx"]))),
            sampler=SubsetRandomSampler(best_overall["val_idx"]),
            num_workers=cfg.workers,
            collate_fn=collate_pool,
            pin_memory=(device.type == "cuda"),
        )
        dummy = torch.tensor([0.0])
        normalizer = Normalizer(dummy)
        normalizer.load_state_dict(best_ckpt["normalizer"])
        model = CrystalGraphConvNet(
            orig_atom_fea_len,
            nbr_fea_len,
            atom_fea_len=cfg.atom_fea_len,
            n_conv=cfg.n_conv,
            h_fea_len=cfg.h_fea_len,
            n_h=cfg.n_h,
        ).to(device)
        model.load_state_dict(best_ckpt["state_dict"])
        train_csv = os.path.join(cfg.out_dir, "train_results.csv")
        val_csv = os.path.join(cfg.out_dir, "vali_results.csv")
        test_csv = os.path.join(cfg.out_dir, "test_results.csv")
        log_path = os.path.join(cfg.out_dir, "log.dat")
        train_metrics = evaluate(
            best_train_loader,
            model,
            normalizer,
            device,
            collect=True,
            out_csv=train_csv,
        )
        val_metrics = evaluate(
            best_val_loader, model, normalizer, device, collect=True, out_csv=val_csv
        )
        test_metrics = evaluate(
            test_loader, model, normalizer, device, collect=True, out_csv=test_csv
        )
        with open(log_path, "w", encoding="utf-8") as logf:
            model_str = str(model)
            print(model_str)
            logf.write("===== Model Architecture =====\n")
            logf.write(model_str + "\n")
            n_params = sum(p.numel() for p in model.parameters())
            n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
            logf.write(f"Total params: {n_params}\n")
            logf.write(f"Trainable params: {n_trainable}\n")
            logf.write("==============================\n")

            def line(name, m):
                s = f"{name}: MAE={m['mae']:.6f} RMSE={m['rmse']:.6f} R2={m['r2']:.6f}"
                print(s)
                logf.write(s + "\n")

            line(f'Train (best fold={best_overall["fold"]})', train_metrics)
            line("Val", val_metrics)
            line("Test", test_metrics)
            runtime = time.time() - t0
            logf.write(
                f'BestFold={best_overall["fold"]}  BestValRMSE={best_overall["best_val_rmse"]:.6f}\n'
            )
            logf.write(f"Runtime(s): {runtime:.3f}\n")
            print(
                f'BestFold={best_overall["fold"]}  BestValRMSE={best_overall["best_val_rmse"]:.6f}'
            )
            print(f"Runtime(s): {runtime:.3f}")
        return {
            "best_val_rmse": best_overall["best_val_rmse"],
            "best_fold": best_overall["fold"],
            "train": train_metrics,
            "val": val_metrics,
            "test": test_metrics,
        }


def cdf(x):
    return x if (x is None or isinstance(x, int)) else None


# ----------------------------- Optuna -----------------------------
def run_optuna():
    study = optuna.create_study(direction="minimize")

    def objective(trial: "optuna.trial.Trial"):
        lr = 1e-3
        optim_name = "Adam"
        # n_conv = trial.suggest_categorical('n_conv', [3, 4, 5])
        # h_fea_len = trial.suggest_categorical('h_fea_len', [96, 128, 256])
        # atom_fea_len = trial.suggest_categorical('atom_fea_len', [32, 64, 96])
        # n_h = trial.suggest_categorical('n_h', [1, 2])
        atom_fea_len = trial.suggest_int("atom_fea_len", 48, 95, step=8)
        h_fea_len = trial.suggest_int("h_fea_len", 96, 191, step=16)
        pair = trial.suggest_categorical(
            "pair", [(1, 3), (1, 2), (1, 4), (2, 3), (2, 4), (2, 5)]
        )
        n_h, n_conv = pair
        batch_size = 256
        out_dir = os.path.join(SCRIPT_DIR, "runs", f"optuna_trial_{trial.number}")
        os.makedirs(out_dir, exist_ok=True)
        cfg = Config(
            struct_dir=STRUCT_DIR,
            atom_init_path=ATOM_INIT_PATH,
            meta_file=META_FILE,
            id_col=ID_COL,
            target_col=TARGET_COL,
            train_ratio=train_ratio,
            val_ratio=val_ratio,
            test_ratio=test_ratio,
            epochs=epochs,
            batch_size=batch_size,
            lr=lr,
            optim=optim_name,
            lr_milestones=[100, 150],
            atom_fea_len=atom_fea_len,
            h_fea_len=h_fea_len,
            n_conv=n_conv,
            n_h=n_h,
            print_freq=print_freq,
            use_cuda=True,
            early_stop_patience=patience,
            out_dir=out_dir,
        )
        result = run_experiment(cfg, trial_tag=f"trial_{trial.number}")
        best = result["best_val_rmse"]
        trial.report(best, step=0)
        trial_log = os.path.join(out_dir, "log.dat")
        with open(trial_log, "a", encoding="utf-8") as f:
            f.write(f"Best value: {best}\n")
            f.write("Best params:\n")
            f.write(f"  atom_fea_len: {atom_fea_len}\n")
            f.write(f"  h_fea_len: {h_fea_len}\n")
            f.write(f"  pair: ({n_h}, {n_conv})\n")
        return best

    study.optimize(objective, n_trials=OPTUNA_TRIALS, show_progress_bar=True)
    best_trial = study.best_trial
    best_num = best_trial.number
    print("Best value:", study.best_value)
    print("Best trial:", f"optuna_trial_{best_num}")
    print("Best params:", study.best_params)
    os.makedirs(os.path.join(SCRIPT_DIR, "runs"), exist_ok=True)
    summary_log = os.path.join(SCRIPT_DIR, "runs", "log.dat")
    with open(summary_log, "a", encoding="utf-8") as f:
        f.write("===== Optuna Summary =====\n")
        f.write(f"Best value: {study.best_value}\n")
        f.write(f"Best trial: optuna_trial_{best_num}\n")
        f.write("Best params:\n")
        for k, v in study.best_params.items():
            f.write(f"  {k}: {v}\n")
    return study


# ----------------------------- Entry -----------------------------
if __name__ == "__main__":
    if USE_OPTUNA:
        run_optuna()
    else:
        cfg = Config(
            struct_dir=STRUCT_DIR,
            atom_init_path=ATOM_INIT_PATH,
            meta_file=META_FILE,
            id_col=ID_COL,
            target_col=TARGET_COL,
            train_ratio=train_ratio,
            val_ratio=val_ratio,
            test_ratio=test_ratio,
            epochs=epochs,
            batch_size=batch_size,
            lr=lr,
            optim="Adam",
            lr_milestones=[100, 150],
            atom_fea_len=64,
            h_fea_len=128,
            n_conv=3,
            n_h=1,
            print_freq=print_freq,
            use_cuda=True,
            early_stop_patience=patience,
            out_dir=os.path.join(SCRIPT_DIR, "outputs"),
        )
        run_experiment(cfg)
